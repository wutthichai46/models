{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c69c50",
   "metadata": {},
   "source": [
    "# Cloud interoperability\n",
    "\n",
    "> This sample connects services between AzureML, Amazon WS and Google CP.\n",
    "\n",
    "Our goal is to train a Machine Learning model for credit cards. By now, we have all we need in our GitHub repository, we recommend cloning the data_connector repository and follow all the next instructions.\n",
    "\n",
    "First of all, you need an active account on GCP, AWS and AzureML. Once you have an account for each one, you need some requirements to accomplish this sample, these requirements are related with configurations and values you need to connect to the cloud. \n",
    "If you want to know more, review [GCP Readme](GCP_README.md), [AWS Readme](AWS_README.md) and [AzureML readme](AZUREML_README.md)\n",
    "\n",
    "**Change your kernel to _intel_sample_env_ at the top right of this document**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe1fe2",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "Our experiment consists in a simple interconnection between 3 cloud providers: GCP, AWS and Azure. \n",
    "Here we will make a use case simulation where you, a data scientist, have a local file with data about credit card behaviors, and you want to save this info into GCP Big Query to extract it later as many times as you want. \n",
    "Also, we will run a Machine Learning process in our local machine to get a model to know who can receive a credit card. \n",
    "\n",
    "But this work is made to see the light of the sun, so we will configure and run our training code into cloud cluster computers on AzureML. \n",
    "\n",
    "After all of this, we need to create versions of our work, so we should have versions of experiments' results and save them in a safe place, we will use an AWS bucket. Then we can get back to previous states of our models just by loading them.\n",
    "\n",
    "\n",
    "| Before running use ```gcloud auth login```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f9432",
   "metadata": {},
   "source": [
    "## GCP load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# GDP Data connector\n",
    "from data_connector.gcp import Connector as GCPConnector\n",
    "from data_connector.gcp import Downloader as GCPDownloader \n",
    "from data_connector.gcp  import Query as GCPQuery\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Pandas \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "sql_types = {np.dtype('int32'): \"INTEGER\", np.dtype('int64'): \"INT64\"}\n",
    "\n",
    "credit_card_name = \"credit_card_clients\"\n",
    "credit_card_file_name = credit_card_name + \".xls\"\n",
    "credit_card_data_df: pd.DataFrame = pd.read_excel(credit_card_file_name, skiprows=[0])\n",
    "schema = []\n",
    "for name in credit_card_data_df.columns:\n",
    "    schema.append(bigquery.SchemaField(name, sql_types[credit_card_data_df.dtypes[name]], mode='REQUIRED'))\n",
    "\n",
    "rows_to_insert = list(credit_card_data_df.itertuples(index=False, name=None))\n",
    "increment = 1000\n",
    "chunks = [rows_to_insert[x:x+increment] for x in range(0, len(rows_to_insert), increment)] #Avoids error 413\n",
    "\n",
    "# Connecting to GCP Big Query\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credentials.json\"\n",
    "gcp_connector = GCPConnector(\"bigquery\")\n",
    "# Create a client \n",
    "bigquery_client = gcp_connector.connect(connection_string=\"<your project>\")\n",
    "gcp_query = GCPQuery(bigquery_client)\n",
    "# Create a data set\n",
    "data_set_name = \"CreditCardClients\"\n",
    "table_name = \"test_query\"\n",
    "\n",
    "gcp_query.create_dataset(data_set_name)\n",
    "gcp_query.create_table(data_set_name, table_name, schema)\n",
    "\n",
    "for elem in chunks:\n",
    "    gcp_query.export_items_to_bq(data_set_name, table_name, elem)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336bfcd",
   "metadata": {},
   "source": [
    "# Azure ML\n",
    "Now we will use our data to train a model locally and then make it run in in AzureML cloud.\n",
    "\n",
    "* Remember, you need a config.json file from AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1956c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.end_run()\n",
    "# Python\n",
    "from pathlib import Path\n",
    "# AZURE\n",
    "# Azure ML\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    AmlCompute,\n",
    "    Environment\n",
    ")\n",
    "\n",
    "# Data connector\n",
    "from data_connector.azure import Connector, MLUploader\n",
    "# Connection to Azure ML\n",
    "ml_client: MLClient = Connector().connect()\n",
    "ml_uploader: MLUploader = MLUploader(ml_client)\n",
    "# Training the model in local\n",
    "path = Path().resolve()\n",
    "training_script = f\"{path}/src/main.py\"\n",
    "training_data = f\"{path}/credit_card_clients.xls\"\n",
    "_test_train_ratio=0.2\n",
    "_learning_rate=0.25\n",
    "_registered_model_name = \"credit_defaults_model\"\n",
    "# Here is our training code ./src/main.py\n",
    "from src.main import main\n",
    "arguments = [\n",
    "        \"--data\", f\"{training_data}\",\n",
    "        \"--test_train_ratio\", f\"{_test_train_ratio}\",\n",
    "        \"--learning_rate\", f\"{_learning_rate}\",\n",
    "        \"--registered_model_name\", f\"{_registered_model_name}\"\n",
    "        ]\n",
    "main(arguments)\n",
    "\n",
    "\n",
    "# Moving to the cloud\n",
    "# AmlCompute \n",
    "# Name assigned to the compute cluster\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "_type = \"amlconpute\"\n",
    "_size = \"STANDARD_DS3_V2\"\n",
    "_min_instances = 0\n",
    "_max_intances = 2\n",
    "_idle_time_before_scale_down = 180\n",
    "_tier = \"Dedicated\"\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure ML compute object with the intended parameters\n",
    "    cpu_cluster = AmlCompute(\n",
    "        name=cpu_compute_target,\n",
    "        # Azure ML Compute is the on-demand VM service\n",
    "        type=_type,\n",
    "        # VM Family\n",
    "        size=_size,\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=_min_instances,\n",
    "        # Nodes in cluster\n",
    "        max_instances=_max_intances,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=_idle_time_before_scale_down,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=_tier,\n",
    "    )\n",
    "    print(\n",
    "        f\"AMLCompute with name {cpu_cluster.name} will be created, with compute size {cpu_cluster.size}\"\n",
    "    )\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    cpu_cluster = ml_uploader.upload(cpu_cluster)\n",
    "\n",
    "#Environment\n",
    "# \n",
    "dependencies_dir = f\"{path}/dependencies/\" \n",
    "custom_env_name =  \"aml-scikit-learn\"\n",
    "_environment_description = (\n",
    "                            \"Custom environment for Credit Card \" \n",
    "                            \"Defaults pipeline\"\n",
    "                        )\n",
    "_environment_tags = {\"scikit-learn\": \"0.24.2\"}\n",
    "_environment_conda_file = f\"{dependencies_dir}/conda.yml\"\n",
    "_environment_docker_image = \"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\"\n",
    "\n",
    "pipeline_job_env: Environment = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=_environment_description,\n",
    "    tags=_environment_tags,\n",
    "    conda_file=_environment_conda_file,\n",
    "    image=_environment_docker_image,\n",
    ")\n",
    "# Upload environment using data connector\n",
    "pipeline_job_env = ml_uploader.upload(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")\n",
    "\n",
    "# Job\n",
    "# Now we will set a job to run on cloud, for this moment we know \n",
    "# the script to training and we have a data set for training\n",
    "# \n",
    "\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "\n",
    "# We shoudl define some extra values to run in cloud systems\n",
    "_code = \".src\"\n",
    "_comand = \"\"\"python main.py --data ${{inputs.data}} \n",
    "        --test_train_ratio ${{inputs.test_train_ratio}} \n",
    "        --learning_rate ${{inputs.learning_rate}} \n",
    "        --registered_model_name ${{inputs.registered_model_name}}\n",
    "        \"\"\"\n",
    "_job_environment=\"aml-scikit-learn@latest\"\n",
    "_job_compute=\"cpu-cluster\"\n",
    "_job_experiment_name=\"train_model_credit_default_prediction\"\n",
    "_job_display_name=\"credit_default_prediction_from_data_connector\"\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        data=Input(\n",
    "            type=\"uri_file\",\n",
    "            path=training_data,\n",
    "        ),\n",
    "        test_train_ratio=_test_train_ratio,\n",
    "        learning_rate=_learning_rate,\n",
    "        registered_model_name=_registered_model_name,\n",
    "    ),\n",
    "    code=\"./src/\",  # location of source code\n",
    "    command=\"python main.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --learning_rate ${{inputs.learning_rate}} --registered_model_name ${{inputs.registered_model_name}}\",\n",
    "    environment=_job_environment,\n",
    "    compute=_job_compute,\n",
    "    experiment_name=_job_experiment_name,\n",
    "    display_name=_job_display_name,\n",
    ")\n",
    "\n",
    "ml_uploader.upload(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427a1b3",
   "metadata": {},
   "source": [
    "# AWS\n",
    "Now we should save local results of our experiements into a bucket in AWS.\n",
    "\n",
    "Here you need to create a bucket to store your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57a05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from data_connector.aws.connector import Connector\n",
    "from data_connector.aws.uploader import Uploader\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# specify a S3 bucket name\n",
    "bucket_name = '<your bucket name>'\n",
    "# create a connector\n",
    "connector = Connector()\n",
    "# connect to aws using default aws access keys\n",
    "conection_object = connector.connect()\n",
    "# Upload a file\n",
    "# create a uploader object using a connection object\n",
    "uploader = Uploader(conection_object)\n",
    "# upload all files from a folder and subfolders\n",
    "now = datetime.now()\n",
    "upload_dir = 'mlruns'\n",
    "for subdir, dirs, files in os.walk(upload_dir):\n",
    "    for file in files:\n",
    "        full_path = os.path.join(subdir, file)\n",
    "        destiny_path = full_path.replace(upload_dir,f'{upload_dir}_{now}')\n",
    "        uploader.upload(bucket_name, full_path, destiny_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_connector",
   "language": "python",
   "name": "data_connector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
