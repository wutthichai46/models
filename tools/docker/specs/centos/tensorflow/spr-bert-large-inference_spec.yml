releases:
  versioned:
    tag_specs:
    - '{spr-bert-large-inference}'
slice_sets:
  spr-bert-large-inference:
    - add_to_name: tf-spr-bert-large-inference
      dockerfile_subdirectory: tensorflow-spr
      args:
        - PACKAGE_NAME=tf-spr-bert-large-inference
        - TENSORFLOW_IMAGE=model-zoo
        - TENSORFLOW_TAG=tensorflow-spr
      partials:
        - tensorflow/tensorflow-base
        - model_package
        - numactl
        - unzip
        - entrypoint
      files:
        - destination: benchmarks/common
          source: benchmarks/common
        - destination: benchmarks/language_modeling/__init__.py
          source: benchmarks/language_modeling/__init__.py
        - destination: benchmarks/language_modeling/tensorflow/__init__.py
          source: benchmarks/language_modeling/tensorflow/__init__.py
        - destination: benchmarks/language_modeling/tensorflow/bert_large/README.md
          source: benchmarks/language_modeling/tensorflow/bert_large/README.md
        - destination: benchmarks/language_modeling/tensorflow/bert_large/__init__.py
          source: benchmarks/language_modeling/tensorflow/bert_large/__init__.py
        - destination: benchmarks/language_modeling/tensorflow/bert_large/inference/__init__.py
          source: benchmarks/language_modeling/tensorflow/bert_large/inference/__init__.py
        - destination: benchmarks/language_modeling/tensorflow/bert_large/inference
          source: benchmarks/language_modeling/tensorflow/bert_large/inference
        - destination: benchmarks/launch_benchmark.py
          source: benchmarks/launch_benchmark.py
        - source: quickstart/language_modeling/tensorflow/bert_large/inference/cpu/inference_realtime.sh
          destination: quickstart/inference_realtime.sh
        - source: quickstart/language_modeling/tensorflow/bert_large/inference/cpu/inference_realtime_weight_sharing.sh
          destination: quickstart/inference_realtime_weight_sharing.sh
        - source: quickstart/language_modeling/tensorflow/bert_large/inference/cpu/inference_throughput.sh
          destination: quickstart/inference_throughput.sh
        - source: quickstart/language_modeling/tensorflow/bert_large/inference/cpu/accuracy.sh
          destination: quickstart/accuracy.sh
        - destination: models/common
          source: models/common
        - destination: models/language_modeling/tensorflow/bert_large/inference
          source: models/language_modeling/tensorflow/bert_large/inference
        - destination: quickstart/common
          source: quickstart/common
      downloads:
      - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/per_channel_opt_int8_bf16_bert.pb
        destination: pretrained_model/bert_large_int8_pretrained_model.pb
      - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/optimized_bf16_bert.pb
        destination: pretrained_model/bert_large_bfloat16_pretrained_model.pb
      - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/fp32_bert_squad.pb
        destination: pretrained_model/bert_large_fp32_pretrained_model.pb
      - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6_1/bert_large_checkpoints.zip
        destination: pretrained_model/bert_large_checkpoints.zip
      wrapper_package_files:
        - source: output/tf-spr-bert-large-inference.tar.gz
          destination: model_packages/tf-spr-bert-large-inference.tar.gz
        - source: quickstart/language_modeling/tensorflow/bert_large/inference/cpu/build.sh
          destination: build.sh
        - source: quickstart/language_modeling/tensorflow/bert_large/inference/cpu/run.sh
          destination: run.sh
        - source: dockerfiles/tensorflow-spr/tf-spr-bert-large-inference.Dockerfile
          destination: tf-spr-bert-large-inference.Dockerfile
        - source: LICENSE
          destination: licenses/LICENSE
        - source: third_party
          destination: licenses/third_party
