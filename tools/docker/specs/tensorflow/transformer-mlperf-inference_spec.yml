releases:
  versioned:
    tag_specs:
    - '{_TAG_PREFIX}{intel-tf}{language-translation}{transformer-mlperf-inference}'
slice_sets:
  transformer-mlperf-inference:
  - add_to_name: -transformer-mlperf-inference
    args:
    - PACKAGE_NAME=transformer-mlperf-inference
    dockerfile_subdirectory: model_containers
    documentation:
      - docs:
        - name: Title
          uri: models/quickstart/language_translation/tensorflow/transformer_mlperf/inference/cpu/.docs/title.md
        - name: Description
          uri: models/quickstart/language_translation/tensorflow/transformer_mlperf/inference/cpu/.docs/description.md
        - name: Datasets
          uri: models/quickstart/language_translation/tensorflow/transformer_mlperf/inference/cpu/.docs/datasets.md
        - name: Quick Start Scripts
          uri: models/quickstart/language_translation/tensorflow/transformer_mlperf/inference/cpu/.docs/quickstart.md
        - name: Bare Metal
          uri: models/quickstart/language_translation/tensorflow/transformer_mlperf/inference/cpu/.docs/baremetal.md
        - name: Resources
          uri: models/quickstart/common/.docs/resources_with_portal_link.md
        name: README.md
        text_replace:
          <mode>: inference
          <model name>: Transformer Language
          <fp32 precision>: FP32
          <fp32 advanced readme link>: fp32/Advanced.md
          <bfloat16 precision>: BFloat16
          <bfloat16 advanced readme link>: bfloat16/Advanced.md
          <int8 precision>: Int8
          <int8 advanced readme link>: int8/Advanced.md
          <use case>: language_translation
          <workload container url>: https://software.intel.com/content/www/us/en/develop/articles/containers/transformer-lt-mlperf-fp32-inference-tensorflow-container.html
        uri: models/benchmarks/language_translation/tensorflow/transformer_mlperf/inference
    downloads: 
    - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/transformer_mlperf_fp32.pb
      destination: transformer_mlperf_fp32.pb
    - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/transformer_mlperf_bloat16.pb
      destination: transformer_mlperf_bfloat16.pb
    - source: https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/transformer_mlperf_int8.pb
      destination: transformer_mlperf_int8.pb
    files:
    - destination: benchmarks/common
      source: benchmarks/common
    - destination: benchmarks/language_translation/__init__.py
      source: benchmarks/language_translation/__init__.py
    - destination: benchmarks/language_translation/tensorflow/__init__.py
      source: benchmarks/language_translation/tensorflow/__init__.py
    - destination: benchmarks/language_translation/tensorflow/transformer_mlperf/inference/__init__.py
      source: benchmarks/language_translation/tensorflow/transformer_mlperf/inference/__init__.py
    - destination: benchmarks/language_translation/tensorflow/transformer_mlperf/inference
      source: benchmarks/language_translation/tensorflow/transformer_mlperf/inference
    - destination: benchmarks/launch_benchmark.py
      source: benchmarks/launch_benchmark.py
    - destination: models/common
      source: models/common
    - destination: models/language_translation/tensorflow/transformer_mlperf/inference
      source: models/language_translation/tensorflow/transformer_mlperf/inference
    - destination: quickstart/common
      source: quickstart/common
    - destination: quickstart
      source: quickstart/language_translation/tensorflow/transformer_mlperf/inference/cpu
    partials:
    - model_package
    - entrypoint
